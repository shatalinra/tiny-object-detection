2020-10-04 23:08:25,615: <INFO> Training autoencoder v1: attempt 0
2020-10-04 23:08:25,616: <INFO> Model: "autoencoder-v1"
2020-10-04 23:08:25,616: <INFO> _________________________________________________________________
2020-10-04 23:08:25,616: <INFO> Layer (type)                 Output Shape              Param #   
2020-10-04 23:08:25,617: <INFO> =================================================================
2020-10-04 23:08:25,619: <INFO> conv1 (Conv2D)               (None, 32, 32, 4)         52        
2020-10-04 23:08:25,619: <INFO> _________________________________________________________________
2020-10-04 23:08:25,620: <INFO> conv1-leaky (LeakyReLU)      (None, 32, 32, 4)         0         
2020-10-04 23:08:25,620: <INFO> _________________________________________________________________
2020-10-04 23:08:25,622: <INFO> deconv1 (Conv2DTranspose)    (None, 64, 64, 3)         51        
2020-10-04 23:08:25,622: <INFO> _________________________________________________________________
2020-10-04 23:08:25,624: <INFO> deconv1-sigmoid (Activation) (None, 64, 64, 3)         0         
2020-10-04 23:08:25,624: <INFO> =================================================================
2020-10-04 23:08:25,625: <INFO> Total params: 103
2020-10-04 23:08:25,626: <INFO> Trainable params: 103
2020-10-04 23:08:25,626: <INFO> Non-trainable params: 0
2020-10-04 23:08:25,627: <INFO> _________________________________________________________________
2020-10-04 23:08:28,626: <INFO> Epoch 0, loss 0.015732, change 0.015732, grad norm 0.008577
2020-10-04 23:08:32,639: <INFO> Epoch 20, loss 0.000473, change 0.015259, grad norm 0.000280
2020-10-04 23:08:36,554: <INFO> Epoch 40, loss 0.000435, change 0.000038, grad norm 0.000214
2020-10-04 23:08:40,436: <INFO> Epoch 60, loss 0.000420, change 0.000015, grad norm 0.000148
2020-10-04 23:08:44,356: <INFO> Epoch 80, loss 0.000318, change 0.000102, grad norm 0.000532
2020-10-04 23:08:48,279: <INFO> Epoch 100, loss 0.000207, change 0.000111, grad norm 0.000875
2020-10-04 23:08:52,245: <INFO> Epoch 120, loss 0.000167, change 0.000040, grad norm 0.001001
2020-10-04 23:08:56,331: <INFO> Epoch 140, loss 0.000111, change 0.000056, grad norm 0.000949
2020-10-04 23:09:00,766: <INFO> Epoch 160, loss 0.000083, change 0.000027, grad norm 0.000531
2020-10-04 23:09:05,835: <INFO> Epoch 180, loss 0.000078, change 0.000006, grad norm 0.000415
2020-10-04 23:09:10,054: <INFO> Epoch 200, loss 0.000077, change 0.000001, grad norm 0.000491
2020-10-04 23:09:14,042: <INFO> Epoch 220, loss 0.000077, change -0.000000, grad norm 0.000856
2020-10-04 23:09:15,294: <INFO> Training autoencoder v1: attempt 1
2020-10-04 23:09:15,295: <INFO> Model: "autoencoder-v1"
2020-10-04 23:09:15,295: <INFO> _________________________________________________________________
2020-10-04 23:09:15,295: <INFO> Layer (type)                 Output Shape              Param #   
2020-10-04 23:09:15,296: <INFO> =================================================================
2020-10-04 23:09:15,296: <INFO> conv1 (Conv2D)               (None, 32, 32, 4)         52        
2020-10-04 23:09:15,297: <INFO> _________________________________________________________________
2020-10-04 23:09:15,297: <INFO> conv1-leaky (LeakyReLU)      (None, 32, 32, 4)         0         
2020-10-04 23:09:15,298: <INFO> _________________________________________________________________
2020-10-04 23:09:15,298: <INFO> deconv1 (Conv2DTranspose)    (None, 64, 64, 3)         51        
2020-10-04 23:09:15,299: <INFO> _________________________________________________________________
2020-10-04 23:09:15,300: <INFO> deconv1-sigmoid (Activation) (None, 64, 64, 3)         0         
2020-10-04 23:09:15,300: <INFO> =================================================================
2020-10-04 23:09:15,302: <INFO> Total params: 103
2020-10-04 23:09:15,302: <INFO> Trainable params: 103
2020-10-04 23:09:15,302: <INFO> Non-trainable params: 0
2020-10-04 23:09:15,303: <INFO> _________________________________________________________________
2020-10-04 23:09:16,149: <INFO> Epoch 0, loss 0.014520, change 0.014520, grad norm 0.007331
2020-10-04 23:09:20,131: <INFO> Epoch 20, loss 0.000406, change 0.014114, grad norm 0.000559
2020-10-04 23:09:24,310: <INFO> Epoch 40, loss 0.000218, change 0.000188, grad norm 0.001360
2020-10-04 23:09:28,297: <INFO> Epoch 60, loss 0.000157, change 0.000061, grad norm 0.002974
2020-10-04 23:09:32,158: <INFO> Epoch 80, loss 0.000150, change 0.000007, grad norm 0.002331
2020-10-04 23:09:36,032: <INFO> Epoch 100, loss 0.000149, change 0.000001, grad norm 0.002443
2020-10-04 23:09:39,897: <INFO> Epoch 120, loss 0.000146, change 0.000003, grad norm 0.001833
2020-10-04 23:09:43,795: <INFO> Epoch 140, loss 0.000144, change 0.000002, grad norm 0.001484
2020-10-04 23:09:47,828: <INFO> Epoch 160, loss 0.000141, change 0.000003, grad norm 0.001381
2020-10-04 23:09:51,775: <INFO> Epoch 180, loss 0.000134, change 0.000007, grad norm 0.000302
2020-10-04 23:09:55,758: <INFO> Epoch 200, loss 0.000128, change 0.000006, grad norm 0.001517
2020-10-04 23:10:00,141: <INFO> Epoch 220, loss 0.000121, change 0.000007, grad norm 0.002210
2020-10-04 23:10:04,550: <INFO> Epoch 240, loss 0.000103, change 0.000017, grad norm 0.000527
2020-10-04 23:10:08,868: <INFO> Epoch 260, loss 0.000092, change 0.000012, grad norm 0.000459
2020-10-04 23:10:13,963: <INFO> Epoch 280, loss 0.000091, change 0.000000, grad norm 0.003147
2020-10-04 23:10:18,948: <INFO> Epoch 300, loss 0.000078, change 0.000013, grad norm 0.000351
2020-10-04 23:10:23,104: <INFO> Epoch 320, loss 0.000077, change 0.000000, grad norm 0.001319
2020-10-04 23:10:27,074: <INFO> Epoch 340, loss 0.000079, change -0.000002, grad norm 0.002000
2020-10-04 23:10:29,599: <INFO> Training autoencoder v1: attempt 2
2020-10-04 23:10:29,599: <INFO> Model: "autoencoder-v1"
2020-10-04 23:10:29,600: <INFO> _________________________________________________________________
2020-10-04 23:10:29,600: <INFO> Layer (type)                 Output Shape              Param #   
2020-10-04 23:10:29,600: <INFO> =================================================================
2020-10-04 23:10:29,601: <INFO> conv1 (Conv2D)               (None, 32, 32, 4)         52        
2020-10-04 23:10:29,602: <INFO> _________________________________________________________________
2020-10-04 23:10:29,602: <INFO> conv1-leaky (LeakyReLU)      (None, 32, 32, 4)         0         
2020-10-04 23:10:29,603: <INFO> _________________________________________________________________
2020-10-04 23:10:29,603: <INFO> deconv1 (Conv2DTranspose)    (None, 64, 64, 3)         51        
2020-10-04 23:10:29,604: <INFO> _________________________________________________________________
2020-10-04 23:10:29,605: <INFO> deconv1-sigmoid (Activation) (None, 64, 64, 3)         0         
2020-10-04 23:10:29,606: <INFO> =================================================================
2020-10-04 23:10:29,608: <INFO> Total params: 103
2020-10-04 23:10:29,608: <INFO> Trainable params: 103
2020-10-04 23:10:29,609: <INFO> Non-trainable params: 0
2020-10-04 23:10:29,609: <INFO> _________________________________________________________________
2020-10-04 23:10:30,479: <INFO> Epoch 0, loss 0.015416, change 0.015416, grad norm 0.005995
2020-10-04 23:10:34,466: <INFO> Epoch 20, loss 0.000428, change 0.014989, grad norm 0.000857
2020-10-04 23:10:38,504: <INFO> Epoch 40, loss 0.000404, change 0.000023, grad norm 0.000304
2020-10-04 23:10:42,570: <INFO> Epoch 60, loss 0.000326, change 0.000078, grad norm 0.000271
2020-10-04 23:10:46,582: <INFO> Epoch 80, loss 0.000207, change 0.000119, grad norm 0.000796
2020-10-04 23:10:50,519: <INFO> Epoch 100, loss 0.000169, change 0.000038, grad norm 0.000379
2020-10-04 23:10:54,448: <INFO> Epoch 120, loss 0.000153, change 0.000016, grad norm 0.001046
2020-10-04 23:10:58,517: <INFO> Epoch 140, loss 0.000150, change 0.000004, grad norm 0.001378
2020-10-04 23:11:02,443: <INFO> Epoch 160, loss 0.000144, change 0.000005, grad norm 0.000288
2020-10-04 23:11:06,450: <INFO> Epoch 180, loss 0.000137, change 0.000007, grad norm 0.000521
2020-10-04 23:11:10,309: <INFO> Epoch 200, loss 0.000119, change 0.000019, grad norm 0.000275
2020-10-04 23:11:14,198: <INFO> Epoch 220, loss 0.000092, change 0.000027, grad norm 0.000461
2020-10-04 23:11:18,090: <INFO> Epoch 240, loss 0.000079, change 0.000012, grad norm 0.000706
2020-10-04 23:11:21,965: <INFO> Epoch 260, loss 0.000076, change 0.000003, grad norm 0.000241
2020-10-04 23:11:25,833: <INFO> Epoch 280, loss 0.000075, change 0.000001, grad norm 0.000441
2020-10-04 23:11:29,691: <INFO> Epoch 300, loss 0.000078, change -0.000003, grad norm 0.001483
2020-10-04 23:11:31,192: <INFO> Assets written to: model\v1\assets
2020-10-04 23:11:31,236: <INFO> Best loss is 0.000076
2020-10-04 23:11:45,117: <INFO> max loss is 0.020674
