2020-10-04 23:50:41,739: <INFO> Training autoencoder v2: attempt 0
2020-10-04 23:50:41,740: <INFO> Model: "autoencoder-v2"
2020-10-04 23:50:41,741: <INFO> _________________________________________________________________
2020-10-04 23:50:41,742: <INFO> Layer (type)                 Output Shape              Param #   
2020-10-04 23:50:41,743: <INFO> =================================================================
2020-10-04 23:50:41,745: <INFO> conv1 (Conv2D)               (None, 32, 32, 4)         52        
2020-10-04 23:50:41,745: <INFO> _________________________________________________________________
2020-10-04 23:50:41,747: <INFO> conv1-leaky (LeakyReLU)      (None, 32, 32, 4)         0         
2020-10-04 23:50:41,748: <INFO> _________________________________________________________________
2020-10-04 23:50:41,749: <INFO> conv2 (Conv2D)               (None, 16, 16, 10)        170       
2020-10-04 23:50:41,750: <INFO> _________________________________________________________________
2020-10-04 23:50:41,750: <INFO> conv2-leaky (LeakyReLU)      (None, 16, 16, 10)        0         
2020-10-04 23:50:41,751: <INFO> _________________________________________________________________
2020-10-04 23:50:41,752: <INFO> deconv2 (Conv2DTranspose)    (None, 32, 32, 4)         164       
2020-10-04 23:50:41,752: <INFO> _________________________________________________________________
2020-10-04 23:50:41,753: <INFO> deconv2-leaky (LeakyReLU)    (None, 32, 32, 4)         0         
2020-10-04 23:50:41,753: <INFO> _________________________________________________________________
2020-10-04 23:50:41,754: <INFO> deconv1 (Conv2DTranspose)    (None, 64, 64, 3)         51        
2020-10-04 23:50:41,755: <INFO> _________________________________________________________________
2020-10-04 23:50:41,756: <INFO> deconv1-sigmoid (Activation) (None, 64, 64, 3)         0         
2020-10-04 23:50:41,756: <INFO> =================================================================
2020-10-04 23:50:41,758: <INFO> Total params: 437
2020-10-04 23:50:41,759: <INFO> Trainable params: 334
2020-10-04 23:50:41,760: <INFO> Non-trainable params: 103
2020-10-04 23:50:41,760: <INFO> _________________________________________________________________
2020-10-04 23:50:44,756: <INFO> Epoch 0, loss 0.051622, change 0.051622, grad norm 0.021300
2020-10-04 23:50:48,717: <INFO> Epoch 20, loss 0.000666, change 0.050956, grad norm 0.000256
2020-10-04 23:50:52,629: <INFO> Epoch 40, loss 0.000582, change 0.000084, grad norm 0.000396
2020-10-04 23:50:56,521: <INFO> Epoch 60, loss 0.000504, change 0.000078, grad norm 0.000570
2020-10-04 23:51:00,430: <INFO> Epoch 80, loss 0.000442, change 0.000062, grad norm 0.001322
2020-10-04 23:51:04,350: <INFO> Epoch 100, loss 0.000403, change 0.000039, grad norm 0.001018
2020-10-04 23:51:08,287: <INFO> Epoch 120, loss 0.000381, change 0.000022, grad norm 0.000551
2020-10-04 23:51:12,171: <INFO> Epoch 140, loss 0.000355, change 0.000026, grad norm 0.000473
2020-10-04 23:51:16,094: <INFO> Epoch 160, loss 0.000304, change 0.000051, grad norm 0.000683
2020-10-04 23:51:20,021: <INFO> Epoch 180, loss 0.000264, change 0.000039, grad norm 0.001381
2020-10-04 23:51:23,945: <INFO> Epoch 200, loss 0.000239, change 0.000026, grad norm 0.000418
2020-10-04 23:51:27,901: <INFO> Epoch 220, loss 0.000216, change 0.000023, grad norm 0.000885
2020-10-04 23:51:31,816: <INFO> Epoch 240, loss 0.000198, change 0.000018, grad norm 0.000689
2020-10-04 23:51:35,754: <INFO> Epoch 260, loss 0.000187, change 0.000012, grad norm 0.000541
2020-10-04 23:51:39,658: <INFO> Epoch 280, loss 0.000180, change 0.000007, grad norm 0.000473
2020-10-04 23:51:43,579: <INFO> Epoch 300, loss 0.000180, change 0.000000, grad norm 0.002426
2020-10-04 23:51:47,506: <INFO> Epoch 320, loss 0.000171, change 0.000009, grad norm 0.001811
2020-10-04 23:51:51,431: <INFO> Epoch 340, loss 0.000163, change 0.000008, grad norm 0.001661
2020-10-04 23:51:55,365: <INFO> Epoch 360, loss 0.000154, change 0.000009, grad norm 0.001106
2020-10-04 23:51:59,285: <INFO> Epoch 380, loss 0.000148, change 0.000006, grad norm 0.000465
2020-10-04 23:52:03,208: <INFO> Epoch 400, loss 0.000146, change 0.000003, grad norm 0.000887
2020-10-04 23:52:07,127: <INFO> Epoch 420, loss 0.000142, change 0.000003, grad norm 0.000514
2020-10-04 23:52:11,044: <INFO> Epoch 440, loss 0.000141, change 0.000002, grad norm 0.000879
2020-10-04 23:52:14,960: <INFO> Epoch 460, loss 0.000137, change 0.000003, grad norm 0.000192
2020-10-04 23:52:18,868: <INFO> Epoch 480, loss 0.000135, change 0.000002, grad norm 0.000632
2020-10-04 23:52:22,681: <INFO> Training autoencoder v2: attempt 1
2020-10-04 23:52:22,681: <INFO> Model: "autoencoder-v2"
2020-10-04 23:52:22,682: <INFO> _________________________________________________________________
2020-10-04 23:52:22,682: <INFO> Layer (type)                 Output Shape              Param #   
2020-10-04 23:52:22,683: <INFO> =================================================================
2020-10-04 23:52:22,684: <INFO> conv1 (Conv2D)               (None, 32, 32, 4)         52        
2020-10-04 23:52:22,685: <INFO> _________________________________________________________________
2020-10-04 23:52:22,686: <INFO> conv1-leaky (LeakyReLU)      (None, 32, 32, 4)         0         
2020-10-04 23:52:22,686: <INFO> _________________________________________________________________
2020-10-04 23:52:22,687: <INFO> conv2 (Conv2D)               (None, 16, 16, 10)        170       
2020-10-04 23:52:22,688: <INFO> _________________________________________________________________
2020-10-04 23:52:22,689: <INFO> conv2-leaky (LeakyReLU)      (None, 16, 16, 10)        0         
2020-10-04 23:52:22,689: <INFO> _________________________________________________________________
2020-10-04 23:52:22,690: <INFO> deconv2 (Conv2DTranspose)    (None, 32, 32, 4)         164       
2020-10-04 23:52:22,691: <INFO> _________________________________________________________________
2020-10-04 23:52:22,692: <INFO> deconv2-leaky (LeakyReLU)    (None, 32, 32, 4)         0         
2020-10-04 23:52:22,692: <INFO> _________________________________________________________________
2020-10-04 23:52:22,695: <INFO> deconv1 (Conv2DTranspose)    (None, 64, 64, 3)         51        
2020-10-04 23:52:22,695: <INFO> _________________________________________________________________
2020-10-04 23:52:22,698: <INFO> deconv1-sigmoid (Activation) (None, 64, 64, 3)         0         
2020-10-04 23:52:22,698: <INFO> =================================================================
2020-10-04 23:52:22,700: <INFO> Total params: 437
2020-10-04 23:52:22,701: <INFO> Trainable params: 334
2020-10-04 23:52:22,702: <INFO> Non-trainable params: 103
2020-10-04 23:52:22,703: <INFO> _________________________________________________________________
2020-10-04 23:52:23,642: <INFO> Epoch 0, loss 0.033540, change 0.033540, grad norm 0.028845
2020-10-04 23:52:27,583: <INFO> Epoch 20, loss 0.000607, change 0.032933, grad norm 0.000230
2020-10-04 23:52:31,526: <INFO> Epoch 40, loss 0.000482, change 0.000125, grad norm 0.000276
2020-10-04 23:52:35,487: <INFO> Epoch 60, loss 0.000404, change 0.000078, grad norm 0.000187
2020-10-04 23:52:39,422: <INFO> Epoch 80, loss 0.000356, change 0.000048, grad norm 0.000673
2020-10-04 23:52:43,349: <INFO> Epoch 100, loss 0.000269, change 0.000087, grad norm 0.000757
2020-10-04 23:52:47,275: <INFO> Epoch 120, loss 0.000250, change 0.000019, grad norm 0.001644
2020-10-04 23:52:51,213: <INFO> Epoch 140, loss 0.000232, change 0.000018, grad norm 0.000828
2020-10-04 23:52:55,158: <INFO> Epoch 160, loss 0.000216, change 0.000016, grad norm 0.001255
2020-10-04 23:52:59,105: <INFO> Epoch 180, loss 0.000211, change 0.000005, grad norm 0.003621
2020-10-04 23:53:03,040: <INFO> Epoch 200, loss 0.000189, change 0.000022, grad norm 0.000534
2020-10-04 23:53:06,975: <INFO> Epoch 220, loss 0.000181, change 0.000008, grad norm 0.002011
2020-10-04 23:53:10,911: <INFO> Epoch 240, loss 0.000168, change 0.000013, grad norm 0.001057
2020-10-04 23:53:14,845: <INFO> Epoch 260, loss 0.000161, change 0.000006, grad norm 0.002593
2020-10-04 23:53:18,759: <INFO> Epoch 280, loss 0.000147, change 0.000014, grad norm 0.000654
2020-10-04 23:53:22,690: <INFO> Epoch 300, loss 0.000142, change 0.000005, grad norm 0.001534
2020-10-04 23:53:26,626: <INFO> Epoch 320, loss 0.000141, change 0.000001, grad norm 0.002236
2020-10-04 23:53:30,596: <INFO> Epoch 340, loss 0.000130, change 0.000011, grad norm 0.000985
2020-10-04 23:53:34,583: <INFO> Epoch 360, loss 0.000129, change 0.000001, grad norm 0.001663
2020-10-04 23:53:38,546: <INFO> Epoch 380, loss 0.000127, change 0.000002, grad norm 0.001928
2020-10-04 23:53:42,467: <INFO> Epoch 400, loss 0.000124, change 0.000003, grad norm 0.001137
2020-10-04 23:53:46,393: <INFO> Epoch 420, loss 0.000123, change 0.000001, grad norm 0.001120
2020-10-04 23:53:49,407: <INFO> Training autoencoder v2: attempt 2
2020-10-04 23:53:49,408: <INFO> Model: "autoencoder-v2"
2020-10-04 23:53:49,408: <INFO> _________________________________________________________________
2020-10-04 23:53:49,409: <INFO> Layer (type)                 Output Shape              Param #   
2020-10-04 23:53:49,409: <INFO> =================================================================
2020-10-04 23:53:49,412: <INFO> conv1 (Conv2D)               (None, 32, 32, 4)         52        
2020-10-04 23:53:49,413: <INFO> _________________________________________________________________
2020-10-04 23:53:49,414: <INFO> conv1-leaky (LeakyReLU)      (None, 32, 32, 4)         0         
2020-10-04 23:53:49,414: <INFO> _________________________________________________________________
2020-10-04 23:53:49,416: <INFO> conv2 (Conv2D)               (None, 16, 16, 10)        170       
2020-10-04 23:53:49,416: <INFO> _________________________________________________________________
2020-10-04 23:53:49,417: <INFO> conv2-leaky (LeakyReLU)      (None, 16, 16, 10)        0         
2020-10-04 23:53:49,418: <INFO> _________________________________________________________________
2020-10-04 23:53:49,419: <INFO> deconv2 (Conv2DTranspose)    (None, 32, 32, 4)         164       
2020-10-04 23:53:49,419: <INFO> _________________________________________________________________
2020-10-04 23:53:49,421: <INFO> deconv2-leaky (LeakyReLU)    (None, 32, 32, 4)         0         
2020-10-04 23:53:49,421: <INFO> _________________________________________________________________
2020-10-04 23:53:49,423: <INFO> deconv1 (Conv2DTranspose)    (None, 64, 64, 3)         51        
2020-10-04 23:53:49,423: <INFO> _________________________________________________________________
2020-10-04 23:53:49,424: <INFO> deconv1-sigmoid (Activation) (None, 64, 64, 3)         0         
2020-10-04 23:53:49,424: <INFO> =================================================================
2020-10-04 23:53:49,426: <INFO> Total params: 437
2020-10-04 23:53:49,427: <INFO> Trainable params: 334
2020-10-04 23:53:49,428: <INFO> Non-trainable params: 103
2020-10-04 23:53:49,428: <INFO> _________________________________________________________________
2020-10-04 23:53:50,463: <INFO> Epoch 0, loss 0.061531, change 0.061531, grad norm 0.021344
2020-10-04 23:53:54,383: <INFO> Epoch 20, loss 0.000543, change 0.060988, grad norm 0.000505
2020-10-04 23:53:58,314: <INFO> Epoch 40, loss 0.000486, change 0.000057, grad norm 0.000376
2020-10-04 23:54:02,254: <INFO> Epoch 60, loss 0.000406, change 0.000080, grad norm 0.000218
2020-10-04 23:54:06,175: <INFO> Epoch 80, loss 0.000362, change 0.000044, grad norm 0.000658
2020-10-04 23:54:10,106: <INFO> Epoch 100, loss 0.000314, change 0.000048, grad norm 0.000826
2020-10-04 23:54:14,034: <INFO> Epoch 120, loss 0.000269, change 0.000045, grad norm 0.000861
2020-10-04 23:54:17,976: <INFO> Epoch 140, loss 0.000250, change 0.000018, grad norm 0.000625
2020-10-04 23:54:21,907: <INFO> Epoch 160, loss 0.000242, change 0.000008, grad norm 0.002158
2020-10-04 23:54:25,847: <INFO> Epoch 180, loss 0.000230, change 0.000012, grad norm 0.001227
2020-10-04 23:54:29,779: <INFO> Epoch 200, loss 0.000221, change 0.000009, grad norm 0.000586
2020-10-04 23:54:33,706: <INFO> Epoch 220, loss 0.000223, change -0.000003, grad norm 0.003527
2020-10-04 23:54:37,633: <INFO> Epoch 240, loss 0.000204, change 0.000019, grad norm 0.000337
2020-10-04 23:54:41,558: <INFO> Epoch 260, loss 0.000194, change 0.000010, grad norm 0.001526
2020-10-04 23:54:45,484: <INFO> Epoch 280, loss 0.000179, change 0.000015, grad norm 0.000617
2020-10-04 23:54:49,403: <INFO> Epoch 300, loss 0.000168, change 0.000011, grad norm 0.001334
2020-10-04 23:54:53,346: <INFO> Epoch 320, loss 0.000159, change 0.000009, grad norm 0.001818
2020-10-04 23:54:57,283: <INFO> Epoch 340, loss 0.000151, change 0.000008, grad norm 0.000971
2020-10-04 23:55:01,202: <INFO> Epoch 360, loss 0.000150, change 0.000001, grad norm 0.002103
2020-10-04 23:55:05,141: <INFO> Epoch 380, loss 0.000144, change 0.000006, grad norm 0.000909
2020-10-04 23:55:09,076: <INFO> Epoch 400, loss 0.000143, change 0.000001, grad norm 0.000831
2020-10-04 23:55:12,999: <INFO> Epoch 420, loss 0.000143, change 0.000000, grad norm 0.001582
2020-10-04 23:55:16,928: <INFO> Epoch 440, loss 0.000140, change 0.000003, grad norm 0.000821
2020-10-04 23:55:20,864: <INFO> Epoch 460, loss 0.000142, change -0.000002, grad norm 0.002056
2020-10-04 23:55:24,792: <INFO> Epoch 480, loss 0.000137, change 0.000005, grad norm 0.000559
2020-10-04 23:55:30,517: <INFO> Assets written to: model\v2\assets
2020-10-04 23:55:30,571: <INFO> Best loss is 0.000123
2020-10-04 23:55:57,303: <INFO> max loss is 0.020950
