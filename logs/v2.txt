2020-10-06 20:53:22,586: <INFO> Training autoencoder v2: attempt 0
2020-10-06 20:53:22,587: <INFO> Model: "autoencoder-v2"
2020-10-06 20:53:22,588: <INFO> _________________________________________________________________
2020-10-06 20:53:22,589: <INFO> Layer (type)                 Output Shape              Param #   
2020-10-06 20:53:22,590: <INFO> =================================================================
2020-10-06 20:53:22,591: <INFO> conv1 (Conv2D)               (None, 32, 32, 7)         91        
2020-10-06 20:53:22,591: <INFO> _________________________________________________________________
2020-10-06 20:53:22,593: <INFO> conv1-leaky (LeakyReLU)      (None, 32, 32, 7)         0         
2020-10-06 20:53:22,593: <INFO> _________________________________________________________________
2020-10-06 20:53:22,594: <INFO> conv2 (Conv2D)               (None, 16, 16, 16)        464       
2020-10-06 20:53:22,595: <INFO> _________________________________________________________________
2020-10-06 20:53:22,596: <INFO> conv2-leaky (LeakyReLU)      (None, 16, 16, 16)        0         
2020-10-06 20:53:22,596: <INFO> _________________________________________________________________
2020-10-06 20:53:22,597: <INFO> deconv2 (Conv2DTranspose)    (None, 32, 32, 7)         455       
2020-10-06 20:53:22,598: <INFO> _________________________________________________________________
2020-10-06 20:53:22,599: <INFO> deconv2-leaky (LeakyReLU)    (None, 32, 32, 7)         0         
2020-10-06 20:53:22,599: <INFO> _________________________________________________________________
2020-10-06 20:53:22,601: <INFO> deconv1 (Conv2DTranspose)    (None, 64, 64, 3)         87        
2020-10-06 20:53:22,601: <INFO> _________________________________________________________________
2020-10-06 20:53:22,602: <INFO> deconv1-sigmoid (Activation) (None, 64, 64, 3)         0         
2020-10-06 20:53:22,603: <INFO> =================================================================
2020-10-06 20:53:22,606: <INFO> Total params: 1,097
2020-10-06 20:53:22,606: <INFO> Trainable params: 919
2020-10-06 20:53:22,607: <INFO> Non-trainable params: 178
2020-10-06 20:53:22,608: <INFO> _________________________________________________________________
2020-10-06 20:53:25,531: <INFO> Epoch 0, loss 0.003391, change 0.003391, grad norm 0.013365, lr 0.010000
2020-10-06 20:53:29,540: <INFO> Epoch 20, loss 0.000250, change 0.003142, grad norm 0.002156, lr 0.010000
2020-10-06 20:53:33,693: <INFO> Epoch 40, loss 0.000180, change 0.000069, grad norm 0.002739, lr 0.010000
2020-10-06 20:53:37,758: <INFO> Epoch 60, loss 0.000152, change 0.000028, grad norm 0.002672, lr 0.010000
2020-10-06 20:53:41,775: <INFO> Epoch 80, loss 0.000131, change 0.000021, grad norm 0.003306, lr 0.010000
2020-10-06 20:53:45,880: <INFO> Epoch 100, loss 0.000101, change 0.000031, grad norm 0.000400, lr 0.010000
2020-10-06 20:53:50,860: <INFO> Epoch 120, loss 0.000088, change 0.000012, grad norm 0.000800, lr 0.010000
2020-10-06 20:53:55,384: <INFO> Epoch 140, loss 0.000081, change 0.000007, grad norm 0.001141, lr 0.010000
2020-10-06 20:53:59,633: <INFO> Epoch 160, loss 0.000079, change 0.000002, grad norm 0.003039, lr 0.010000
2020-10-06 20:54:03,672: <INFO> Epoch 180, loss 0.000072, change 0.000007, grad norm 0.002619, lr 0.010000
2020-10-06 20:54:07,712: <INFO> Epoch 200, loss 0.000097, change -0.000026, grad norm 0.006552, lr 0.010000
2020-10-06 20:54:11,703: <INFO> Epoch 220, loss 0.000074, change 0.000023, grad norm 0.003478, lr 0.010000
2020-10-06 20:54:15,705: <INFO> Epoch 240, loss 0.000061, change 0.000013, grad norm 0.000442, lr 0.005000
2020-10-06 20:54:19,711: <INFO> Epoch 260, loss 0.000059, change 0.000002, grad norm 0.000273, lr 0.005000
2020-10-06 20:54:23,721: <INFO> Epoch 280, loss 0.000057, change 0.000002, grad norm 0.000387, lr 0.005000
2020-10-06 20:54:27,697: <INFO> Epoch 300, loss 0.000066, change -0.000008, grad norm 0.003924, lr 0.005000
2020-10-06 20:54:31,675: <INFO> Epoch 320, loss 0.000054, change 0.000012, grad norm 0.000188, lr 0.002500
2020-10-06 20:54:35,641: <INFO> Epoch 340, loss 0.000054, change 0.000001, grad norm 0.000258, lr 0.001250
2020-10-06 20:54:38,915: <INFO> Training autoencoder v2: attempt 1
2020-10-06 20:54:38,916: <INFO> Model: "autoencoder-v2"
2020-10-06 20:54:38,917: <INFO> _________________________________________________________________
2020-10-06 20:54:38,917: <INFO> Layer (type)                 Output Shape              Param #   
2020-10-06 20:54:38,918: <INFO> =================================================================
2020-10-06 20:54:38,919: <INFO> conv1 (Conv2D)               (None, 32, 32, 7)         91        
2020-10-06 20:54:38,920: <INFO> _________________________________________________________________
2020-10-06 20:54:38,921: <INFO> conv1-leaky (LeakyReLU)      (None, 32, 32, 7)         0         
2020-10-06 20:54:38,922: <INFO> _________________________________________________________________
2020-10-06 20:54:38,923: <INFO> conv2 (Conv2D)               (None, 16, 16, 16)        464       
2020-10-06 20:54:38,923: <INFO> _________________________________________________________________
2020-10-06 20:54:38,924: <INFO> conv2-leaky (LeakyReLU)      (None, 16, 16, 16)        0         
2020-10-06 20:54:38,924: <INFO> _________________________________________________________________
2020-10-06 20:54:38,925: <INFO> deconv2 (Conv2DTranspose)    (None, 32, 32, 7)         455       
2020-10-06 20:54:38,925: <INFO> _________________________________________________________________
2020-10-06 20:54:38,926: <INFO> deconv2-leaky (LeakyReLU)    (None, 32, 32, 7)         0         
2020-10-06 20:54:38,926: <INFO> _________________________________________________________________
2020-10-06 20:54:38,927: <INFO> deconv1 (Conv2DTranspose)    (None, 64, 64, 3)         87        
2020-10-06 20:54:38,928: <INFO> _________________________________________________________________
2020-10-06 20:54:38,928: <INFO> deconv1-sigmoid (Activation) (None, 64, 64, 3)         0         
2020-10-06 20:54:38,928: <INFO> =================================================================
2020-10-06 20:54:38,930: <INFO> Total params: 1,097
2020-10-06 20:54:38,931: <INFO> Trainable params: 919
2020-10-06 20:54:38,931: <INFO> Non-trainable params: 178
2020-10-06 20:54:38,932: <INFO> _________________________________________________________________
2020-10-06 20:54:39,837: <INFO> Epoch 0, loss 0.004047, change 0.004047, grad norm 0.014116, lr 0.010000
2020-10-06 20:54:43,847: <INFO> Epoch 20, loss 0.000290, change 0.003757, grad norm 0.001166, lr 0.010000
2020-10-06 20:54:47,920: <INFO> Epoch 40, loss 0.000190, change 0.000101, grad norm 0.001091, lr 0.010000
2020-10-06 20:54:51,952: <INFO> Epoch 60, loss 0.000139, change 0.000051, grad norm 0.000949, lr 0.010000
2020-10-06 20:54:55,968: <INFO> Epoch 80, loss 0.000107, change 0.000032, grad norm 0.000759, lr 0.010000
2020-10-06 20:54:59,978: <INFO> Epoch 100, loss 0.000091, change 0.000016, grad norm 0.000517, lr 0.010000
2020-10-06 20:55:03,987: <INFO> Epoch 120, loss 0.000080, change 0.000011, grad norm 0.000645, lr 0.010000
2020-10-06 20:55:08,009: <INFO> Epoch 140, loss 0.000073, change 0.000007, grad norm 0.001185, lr 0.010000
2020-10-06 20:55:12,010: <INFO> Epoch 160, loss 0.000079, change -0.000006, grad norm 0.004252, lr 0.010000
2020-10-06 20:55:16,001: <INFO> Epoch 180, loss 0.000063, change 0.000016, grad norm 0.001293, lr 0.010000
2020-10-06 20:55:20,001: <INFO> Epoch 200, loss 0.000064, change -0.000001, grad norm 0.002719, lr 0.010000
2020-10-06 20:55:23,999: <INFO> Epoch 220, loss 0.000056, change 0.000008, grad norm 0.001274, lr 0.010000
2020-10-06 20:55:28,000: <INFO> Epoch 240, loss 0.000057, change -0.000001, grad norm 0.001594, lr 0.010000
2020-10-06 20:55:32,067: <INFO> Epoch 260, loss 0.000054, change 0.000002, grad norm 0.001992, lr 0.010000
2020-10-06 20:55:36,059: <INFO> Epoch 280, loss 0.000047, change 0.000008, grad norm 0.001006, lr 0.010000
2020-10-06 20:55:40,060: <INFO> Epoch 300, loss 0.000049, change -0.000003, grad norm 0.002528, lr 0.010000
2020-10-06 20:55:44,070: <INFO> Epoch 320, loss 0.000041, change 0.000009, grad norm 0.000175, lr 0.005000
2020-10-06 20:55:48,081: <INFO> Epoch 340, loss 0.000040, change 0.000000, grad norm 0.000265, lr 0.002500
2020-10-06 20:55:51,193: <INFO> Training autoencoder v2: attempt 2
2020-10-06 20:55:51,194: <INFO> Model: "autoencoder-v2"
2020-10-06 20:55:51,194: <INFO> _________________________________________________________________
2020-10-06 20:55:51,195: <INFO> Layer (type)                 Output Shape              Param #   
2020-10-06 20:55:51,195: <INFO> =================================================================
2020-10-06 20:55:51,197: <INFO> conv1 (Conv2D)               (None, 32, 32, 7)         91        
2020-10-06 20:55:51,198: <INFO> _________________________________________________________________
2020-10-06 20:55:51,199: <INFO> conv1-leaky (LeakyReLU)      (None, 32, 32, 7)         0         
2020-10-06 20:55:51,199: <INFO> _________________________________________________________________
2020-10-06 20:55:51,200: <INFO> conv2 (Conv2D)               (None, 16, 16, 16)        464       
2020-10-06 20:55:51,200: <INFO> _________________________________________________________________
2020-10-06 20:55:51,201: <INFO> conv2-leaky (LeakyReLU)      (None, 16, 16, 16)        0         
2020-10-06 20:55:51,202: <INFO> _________________________________________________________________
2020-10-06 20:55:51,204: <INFO> deconv2 (Conv2DTranspose)    (None, 32, 32, 7)         455       
2020-10-06 20:55:51,204: <INFO> _________________________________________________________________
2020-10-06 20:55:51,205: <INFO> deconv2-leaky (LeakyReLU)    (None, 32, 32, 7)         0         
2020-10-06 20:55:51,206: <INFO> _________________________________________________________________
2020-10-06 20:55:51,207: <INFO> deconv1 (Conv2DTranspose)    (None, 64, 64, 3)         87        
2020-10-06 20:55:51,208: <INFO> _________________________________________________________________
2020-10-06 20:55:51,209: <INFO> deconv1-sigmoid (Activation) (None, 64, 64, 3)         0         
2020-10-06 20:55:51,209: <INFO> =================================================================
2020-10-06 20:55:51,211: <INFO> Total params: 1,097
2020-10-06 20:55:51,211: <INFO> Trainable params: 919
2020-10-06 20:55:51,212: <INFO> Non-trainable params: 178
2020-10-06 20:55:51,213: <INFO> _________________________________________________________________
2020-10-06 20:55:52,202: <INFO> Epoch 0, loss 0.006352, change 0.006352, grad norm 0.025858, lr 0.010000
2020-10-06 20:55:56,191: <INFO> Epoch 20, loss 0.000303, change 0.006049, grad norm 0.000912, lr 0.010000
2020-10-06 20:56:00,302: <INFO> Epoch 40, loss 0.000219, change 0.000084, grad norm 0.001185, lr 0.010000
2020-10-06 20:56:04,274: <INFO> Epoch 60, loss 0.000177, change 0.000042, grad norm 0.001984, lr 0.010000
2020-10-06 20:56:08,246: <INFO> Epoch 80, loss 0.000152, change 0.000025, grad norm 0.001978, lr 0.010000
2020-10-06 20:56:12,218: <INFO> Epoch 100, loss 0.000124, change 0.000028, grad norm 0.001135, lr 0.010000
2020-10-06 20:56:16,204: <INFO> Epoch 120, loss 0.000113, change 0.000011, grad norm 0.002570, lr 0.010000
2020-10-06 20:56:20,182: <INFO> Epoch 140, loss 0.000103, change 0.000009, grad norm 0.003029, lr 0.010000
2020-10-06 20:56:24,146: <INFO> Epoch 160, loss 0.000084, change 0.000019, grad norm 0.000647, lr 0.010000
2020-10-06 20:56:28,113: <INFO> Epoch 180, loss 0.000077, change 0.000007, grad norm 0.001166, lr 0.010000
2020-10-06 20:56:32,081: <INFO> Epoch 200, loss 0.000072, change 0.000005, grad norm 0.001840, lr 0.010000
2020-10-06 20:56:36,047: <INFO> Epoch 220, loss 0.000079, change -0.000007, grad norm 0.003077, lr 0.010000
2020-10-06 20:56:40,126: <INFO> Epoch 240, loss 0.000075, change 0.000004, grad norm 0.004388, lr 0.010000
2020-10-06 20:56:44,108: <INFO> Epoch 260, loss 0.000059, change 0.000016, grad norm 0.001695, lr 0.010000
2020-10-06 20:56:48,077: <INFO> Epoch 280, loss 0.000054, change 0.000005, grad norm 0.001338, lr 0.010000
2020-10-06 20:56:52,100: <INFO> Epoch 300, loss 0.000050, change 0.000004, grad norm 0.000371, lr 0.005000
2020-10-06 20:56:56,122: <INFO> Epoch 320, loss 0.000049, change 0.000001, grad norm 0.000233, lr 0.002500
2020-10-06 20:57:00,099: <INFO> Epoch 340, loss 0.000048, change 0.000001, grad norm 0.000249, lr 0.001250
2020-10-06 20:57:04,065: <INFO> Epoch 360, loss 0.000048, change 0.000000, grad norm 0.000163, lr 0.001000
2020-10-06 20:57:07,772: <INFO> Assets written to: model\v2\assets
2020-10-06 20:57:07,821: <INFO> Best loss is 0.000040
2020-10-06 20:57:36,231: <INFO> max loss is 0.008582
